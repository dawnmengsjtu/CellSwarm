The preceding results rest on a single LLM backbone (DeepSeek). How sensitive are they to model choice? We benchmarked eight decision-making modes, six LLM backbones (DeepSeek, GLM-4-Flash, Qwen-Turbo, Qwen-Plus, Qwen-Max, Kimi-K2.5), hand-coded Rules, and Random, each run with five independent seeds on the TNBC baseline scenario (\figref{fig:5}A).

Two performance tiers separated cleanly. Tier~1 (JS $<$ 0.16): DeepSeek (JS = 0.144 $\pm$ 0.001), Rules (0.146 $\pm$ 0.001), GLM-4-Flash (0.147 $\pm$ 0.002), Qwen-Turbo (0.156 $\pm$ 0.004). Tier~2: Qwen-Plus (0.210 $\pm$ 0.007), Qwen-Max (0.212 $\pm$ 0.018), Kimi-K2.5 (0.219 $\pm$ 0.002). Random diverged most (0.263 $\pm$ 0.017). The tier gap was highly significant (P = 4.4 $\times$ $10^{-21}$, two-sided t-test). Model scale, again, did not predict performance: Qwen-Max underperformed the smaller Qwen-Turbo, reinforcing the conclusion that instruction-following fidelity and structured output compliance, not raw reasoning capacity, govern simulation quality.

Cost matters for a framework intended to run thousands of simulations. Plotting API cost per run against JS divergence (\figref{fig:5}B), DeepSeek and GLM-4-Flash occupied the Pareto-optimal frontier, offering the best accuracy per dollar. Kimi-K2.5 sat at the opposite extreme: most expensive, worst results. These economics informed our choice of DeepSeek as the default backbone.

Reproducibility across seeds was assessed via the coefficient of variation (CV) of JS divergence for each model (\figref{fig:5}C). All Tier-1 models achieved CV $<$ 10\%; Rules showed the tightest spread (CV = 3.5\%), DeepSeek a moderate but acceptable 8.3\%. The low CVs confirm that CellSwarm simulations are reproducible despite the stochasticity inherent in language model sampling.

A model-by-cell-type heatmap of proportion errors exposed a systematic pattern (\figref{fig:5}D): CD8$^+$ T cell overestimation and B cell underestimation appeared across every model, Tier-1 and Tier-2 alike. The bias originates in the simulation dynamics (simplified proliferation mechanics and the absence of humoral immunity modules) rather than in any model-specific reasoning deficit.

To quantify each knowledge base component's contribution, we ran ablation experiments removing one of the three knowledge bases while keeping the other two intact (\figref{fig:5}E). Removing the Cancer Atlas was catastrophic: JS rose to 0.272 $\pm$ 0.031, a 1.9-fold increase over the full model (0.144 $\pm$ 0.001, P = 0.029) and comparable to Random decisions (0.274 $\pm$ 0.007). Without cancer-specific context, the LLM's general biological knowledge alone cannot produce realistic TME dynamics. Removing the Drug Library (JS = 0.143 $\pm$ 0.001) or the Pathway Knowledge Base (JS = 0.144 $\pm$ 0.002) had negligible effects on baseline fidelity, as expected since these components primarily serve treatment and perturbation scenarios.

The same ablation on the Rules engine served as a control (\figref{fig:5}F). Rules was largely insensitive to knowledge base removal for Drug Library, Pathway KB, and Perturbation Atlas (JS $\approx$ 0.146--0.160), but removing the Cancer Atlas also degraded Rules performance (JS = 0.314; Figure~S4), indicating that cancer-type-specific initialization parameters affect both decision-making modes. The contrast nonetheless highlights a fundamental architectural difference: Agent actively retrieves and reasons over knowledge base content; Rules operates on fixed logic independent of external knowledge.

The largest difference between Agent and Rules appeared in cross-cancer simulations (\figref{fig:5}G). Rules produced uniformly low tumor ratios across all six cancer types (TR = 0.04--0.07), with no distinction between hot and cold tumors. Agent showed cancer-type-specific variation that tracked known immunological differences: CRC-MSI-H (TR = 0.07) cleared most aggressively; TNBC (TR = 0.93) regressed least. Cross-cancer immune composition analysis confirmed that Agent preserved cancer-specific immune landscapes while Rules generated near-identical compositions regardless of cancer type (\figref{fig:5}H).

A tier comparison summary across key metrics (\figref{fig:5}I) confirmed Tier-1 models consistently outperforming Tier-2 on JS divergence, compositional accuracy, and reproducibility. Detailed API call statistics for all models appear in Table~S4.

Four of six LLM backbones matched the fidelity of hand-coded rules. The Cancer Atlas knowledge base is the critical component for baseline accuracy. The framework is reproducible, computationally affordable, and does not require frontier-scale models; a mid-tier instruction-following model suffices for biologically realistic simulations.
