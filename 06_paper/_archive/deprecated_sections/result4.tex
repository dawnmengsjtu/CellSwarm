Having demonstrated that CellSwarm can recapitulate baseline TME dynamics (Fig. 2), generalize across cancer types and treatments (Fig. 3), and sense indirect genetic perturbations (Fig. 4), we next assessed the robustness of these results across different LLM backbones, evaluated the contribution of each knowledge base component, and characterized the computational cost of the framework.

We benchmarked eight decision-making modes---six LLM backbones (DeepSeek, GLM-4-Flash, Qwen-Turbo, Qwen-Plus, Qwen-Max, Kimi-K2.5), hand-coded Rules, and Random---each run with five independent seeds on the TNBC baseline scenario (Fig. 5A). The models separated into two distinct performance tiers. Tier 1 comprised DeepSeek (JS = 0.144 $\pm$ 0.001), Rules (0.146 $\pm$ 0.001), GLM-4-Flash (0.147 $\pm$ 0.002), and Qwen-Turbo (0.156 $\pm$ 0.004), all achieving JS divergence below 0.16. Tier 2 comprised Qwen-Plus (0.210 $\pm$ 0.007), Qwen-Max (0.212 $\pm$ 0.018), and Kimi-K2.5 (0.219 $\pm$ 0.002), with JS values approximately 50\% higher. Random decisions produced the worst performance (0.263 $\pm$ 0.017). The difference between tiers was highly significant (P = 4.4 $\times$ 10⁻²¹, two-sided t-test). Notably, model scale did not predict performance: Qwen-Max, the largest model tested, performed worse than the smaller Qwen-Turbo, and Kimi-K2.5 showed the highest JS divergence among all LLM backbones despite being a frontier-class model. This suggests that instruction-following fidelity and structured output compliance---rather than raw reasoning capacity---are the primary determinants of simulation quality.

We evaluated the cost-performance tradeoff by plotting JS divergence against runtime for each LLM backbone (Fig. 5B). DeepSeek achieved the best fidelity (JS = 0.144) at a moderate computational cost (20.5 minutes, 2.8 million tokens per run). GLM-4-Flash offered a compelling alternative with slightly lower fidelity (JS = 0.147) but substantially faster execution (12.8 minutes). Qwen-Turbo was the fastest LLM (8.2 minutes) with acceptable fidelity (JS = 0.156). At the other extreme, Kimi-K2.5 required 49.3 minutes per run---2.4$\times$ slower than DeepSeek---while producing substantially worse results. These findings informed our selection of DeepSeek as the default backbone: it occupies the Pareto-optimal position, offering the best fidelity at reasonable cost.

To quantify the contribution of each knowledge base component, we performed ablation experiments in which one of the three knowledge bases was removed while keeping the other two intact (Fig. 5C). Removing the Cancer Atlas---which provides cancer-type-specific cell proportions, growth rates, and immune infiltration parameters---catastrophically degraded performance (JS = 0.272 $\pm$ 0.031), a 1.9-fold increase over the full model (JS = 0.144 $\pm$ 0.001, P = 0.004). Strikingly, the ablated model performed comparably to Random decisions (JS = 0.274 $\pm$ 0.007), indicating that without cancer-specific context, the LLM's general biological knowledge is insufficient to produce realistic TME dynamics. In contrast, removing the Drug Library (JS = 0.143 $\pm$ 0.001) or the Pathway Knowledge Base (JS = 0.144 $\pm$ 0.002) had negligible effects on baseline simulation fidelity, as expected---these components are primarily relevant during treatment and perturbation scenarios rather than untreated baseline simulations. This result establishes a clear hierarchy of knowledge base importance: the Cancer Atlas is the essential grounding component, while the Drug Library and Pathway KB serve as specialized modules activated by specific experimental conditions.

We assessed reproducibility by computing the coefficient of variation (CV) of the tumor ratio across five seeds for each model (Fig. 5D). All models exhibited CV values below 0.17, indicating that stochastic variation in LLM inference does not produce qualitatively different outcomes across runs. Kimi-K2.5 showed the lowest variability (CV = 0.015), followed by Rules (0.035) and GLM-4-Flash (0.052). DeepSeek achieved a CV of 0.083, indicating moderate but acceptable run-to-run variation. Qwen-Max showed the highest variability (CV = 0.168), consistent with its generally erratic performance across metrics. The low CV values across all models confirm that CellSwarm simulations are reproducible despite the inherent stochasticity of language model sampling.

Finally, we characterized the computational footprint of each model (Fig. 5E). All LLM backbones consumed between 2.8 and 3.2 million tokens per 30-step simulation with 500 cells, corresponding to approximately 3,000 LLM calls per run. The variation in runtime was driven primarily by API latency and model inference speed rather than token count: Kimi-K2.5 consumed 3.2 million tokens but required 49.3 minutes, while Qwen-Turbo consumed 3.0 million tokens in only 8.2 minutes. For the default DeepSeek backbone, a single simulation costs approximately \textyen{}4.2 (US$0.58) at current API pricing, making large-scale parameter sweeps economically feasible.

We note that the current ablation experiments evaluate knowledge base contributions only in the baseline (untreated) scenario. The Drug Library and Pathway KB showed negligible effects here because they are not engaged during baseline simulations. A complete ablation would additionally test Drug Library removal under treatment conditions and Pathway KB removal under perturbation conditions; we leave these condition-specific ablations for future work.

In summary, CellSwarm's performance is robust across multiple LLM backbones, with three of six tested models achieving fidelity comparable to hand-coded rules. The Cancer Atlas knowledge base is the critical component for baseline simulation accuracy, while the Drug Library and Pathway KB become important in treatment and perturbation contexts (Results 2 and 3). The framework is reproducible, computationally affordable, and does not require frontier-scale models---a mid-tier instruction-following model suffices to produce biologically realistic simulations.
